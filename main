import urllib.request
import re
import time

pat2 = '<a target="_blank" href="(http://.*?)"'#获取每页文章

def Use_Proxy(proxy_addr,url):
    try:
        head = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.104 Safari/537.36 Core/1.53.2372.400 QQBrowser/9.5.10771.400'}
        req = urllib.request.Request(url)
        proxy = urllib.request.ProxyHandler({'http':proxy_addr})
        opener = urllib.request.build_opener(proxy,urllib.request.HTTPHandler)
        urllib.request.install_opener(opener)
        data = urllib.request.urlopen(url).read().decode()
        return  data
    except Exception as e:
        print(e)

key = 'python'#搜索关键词
key = urllib.request.quote(key)
for i in range(1,2):#page numbers
    try:
        url = 'http://weixin.sogou.com/weixin?query='+key+'&_sug_type_=&sut=1050&lkt=7%2C1509515878129%2C1509515879148&s_from=input&_sug_=y&type=2&sst0=1509515879250&page='+str(i)+'&ie=utf8&w=01019900&dr=1'
        data2 = Use_Proxy('127.0.0.1:8888',url)#使用代理Ip
        redata = re.compile(pat2,re.S).findall(data2)#获得每页的文章地址
        for j in range(0,len(redata)):
            thisurl = redata[j].replace('amp;','')#need replace修改地址 去掉amp;
            print(thisurl+'\n')
            file = 'E:/python/html/weixin/'+str(i)+str(j)+'.html'
            urllib.request.urlretrieve(thisurl,file)#下载到本地
            time.sleep(3#延时 防止过于频繁的访问
        print(e)
